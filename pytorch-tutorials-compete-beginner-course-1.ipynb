{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorials - Complete Beginner Course\n",
    "\n",
    "- https://www.youtube.com/watch?v=EMXfZB8FVUA&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=1\n",
    "\n",
    "By Python Engineer\n",
    "\n",
    "## Pytorch Tutorial 01 - Installation\n",
    "\n",
    "- https://pytorch.org/;\n",
    "- GPU support: install the cudatoolkit - development environemnt for creating high-performance GPU accelerated applications (need a NVIDIA GPU);\n",
    "- ```conda activate base/pytorch```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3741, 0.0649, 0.1770])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.rand(3) # create torch tensor\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() # does not support GPU :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 02 - Tensor Basics\n",
    "\n",
    "- How to work with tensors and basic operations;\n",
    "- Numpy - arrays and vectors;\n",
    "- Pytorch - tensors;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([19746.7188])\n",
      "tensor([2.0357e+04, 3.0646e-41])\n",
      "tensor([[2.0098e+04, 3.0646e-41, 2.0236e+04],\n",
      "        [3.0646e-41, 8.9683e-44, 0.0000e+00]])\n",
      "tensor([[0.1075, 0.0583],\n",
      "        [0.9569, 0.8155]])\n",
      "tensor([0., 0., 0.])\n",
      "tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.empty(1)\n",
    "print(x) # print empty tensor\n",
    "\n",
    "x = torch.empty(2)\n",
    "print(x)\n",
    "\n",
    "x = torch.empty(2, 3) # create empty tensor with multiple dimension\n",
    "print(x)\n",
    "\n",
    "x = torch.rand(2, 2) # random\n",
    "print(x)\n",
    "\n",
    "x = torch.zeros(3) # zeros\n",
    "print(x)\n",
    "\n",
    "x = torch.ones(3)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float64\n",
      "torch.int32\n",
      "torch.float16\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2)\n",
    "print(x.dtype) # torch.float32 is default value\n",
    "\n",
    "x = torch.ones(2, 2, dtype=torch.double)\n",
    "print(x.dtype)\n",
    "\n",
    "x = torch.ones(2, 2, dtype=torch.int)\n",
    "print(x.dtype) \n",
    "\n",
    "x = torch.ones(2, 2, dtype=torch.float16)\n",
    "print(x.dtype)\n",
    "print(x.size()) # print size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5000, 0.1000, 4.2000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([2.5, 0.1, 4.2])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0907, 0.1031],\n",
      "        [0.0327, 0.3328]])\n",
      "tensor([[0.5270, 0.4406],\n",
      "        [0.9748, 0.2793]])\n",
      "tensor([[0.6176, 0.5437],\n",
      "        [1.0075, 0.6121]])\n",
      "tensor([[1.2353, 1.0873],\n",
      "        [2.0151, 1.2242]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "z = x + y\n",
    "print(z)\n",
    "z = torch.add(z, z) # adding\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In-place operation: operation that changes directly the content of a given Tensor without making a copy. In-place operations in pytorch are always postfixed with a ```_```, like ```.add_()```. Python operations like ```+=``` or ```*=``` are also inplace operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.3669, 1.4498],\n",
      "        [0.4668, 0.2541]])\n",
      "tensor([[-0.5121, -0.8404],\n",
      "        [-0.4568, -0.0030]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "y = torch.rand(2, 2)\n",
    "\n",
    "y.add_(x)\n",
    "print(y)\n",
    "\n",
    "z = x - y;\n",
    "z = torch.sub(x, y) # torch.mul. y.mul_(x), torch.div(x, y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3222, 0.7806, 0.6722],\n",
      "        [0.9685, 0.2158, 0.0512],\n",
      "        [0.3598, 0.1178, 0.4182],\n",
      "        [0.0697, 0.2236, 0.5201],\n",
      "        [0.5012, 0.8890, 0.9663]])\n",
      "tensor([0.3222, 0.9685, 0.3598, 0.0697, 0.5012])\n",
      "tensor([0.9685, 0.2158, 0.0512])\n",
      "tensor(0.3222)\n",
      "0.3221678137779236\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, 3)\n",
    "print(x)\n",
    "print(x[:, 0]) # slicing\n",
    "print(x[1, :])\n",
    "print(x[0, 0])\n",
    "print(x[0, 0].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3222, 0.7806, 0.6722, 0.9685, 0.2158, 0.0512, 0.3598, 0.1178, 0.4182,\n",
      "        0.0697, 0.2236, 0.5201, 0.5012, 0.8890, 0.9663])\n",
      "torch.Size([15])\n",
      "tensor([[0.3222, 0.7806, 0.6722, 0.9685, 0.2158],\n",
      "        [0.0512, 0.3598, 0.1178, 0.4182, 0.0697],\n",
      "        [0.2236, 0.5201, 0.5012, 0.8890, 0.9663]])\n",
      "torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "y = x.view(15)\n",
    "print(y)\n",
    "print(y.size())\n",
    "\n",
    "z = x.view(-1, 5) # reshaping\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = torch.ones(5) # tensor to a numpy array\n",
    "print(a)\n",
    "b = a.numpy()\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "print(a)\n",
    "b = torch.from_numpy(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): # create a tensor and put on GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5, device=device)\n",
    "    y = torch.ones(5)\n",
    "    y = y.to(device)\n",
    "    z = x + y # performed on GPU\n",
    "    print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# pytorch will need to calculate the gradient later in the optimization steps\n",
    "x = torch.ones(5, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 03 - Gradient Calculation With Autograd\n",
    "\n",
    "- ```autograd``` package in pytorch and how to calculate gradients with it (gradients are essential for our model optimization);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3214,  0.2705, -1.5029], requires_grad=True)\n",
      "tensor([1.6786, 2.2705, 0.4971], grad_fn=<AddBackward0>)\n",
      "tensor(5.4801, grad_fn=<MeanBackward0>)\n",
      "tensor([2.2381, 3.0274, 0.6627])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "# 2 inputs in the node: x and 2\n",
    "# node: +\n",
    "# output: y\n",
    "# forward -> calculate y\n",
    "y = x + 2 # computational graph \n",
    "# with backpropagation we can calculate gradients\n",
    "print(y)\n",
    "\n",
    "z = y*y*2\n",
    "z = z.mean() # necessary\n",
    "print(z) # grad_fn=<MulBackward0>\n",
    "\n",
    "z.backward() # dz/dx gradient of z with respect to x\n",
    "print(x.grad) # grad- where gradients are stores\n",
    "\n",
    "# detaisl about chain rule and jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.3214,  0.2705, -1.5029])\n"
     ]
    }
   ],
   "source": [
    "x.requires_grad_(False)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.1547, -0.7095, -1.2912], requires_grad=True)\n",
      "tensor([3.1547, 1.2905, 0.7088])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "with torch.no_grad(): # stop gradient\n",
    "    y = x +2\n",
    "    print(y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3., 3., 3., 3.])\n",
      "tensor([6., 6., 6., 6.])\n",
      "tensor([9., 9., 9., 9.])\n"
     ]
    }
   ],
   "source": [
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "for epoch in range(3):\n",
    "    model_output = (weights*3).sum()\n",
    "    \n",
    "    model_output.backward()\n",
    "    \n",
    "    print(weights.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nweights = torch.ones(4, requires_grad=True)\\n\\noptimizer = torch.optim.SGD(weights, lr=0.01)\\noptimizer.step()\\noptimizer.zero_grad()\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "weights = torch.ones(4, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(weights, lr=0.01)\n",
    "optimizer.step()\n",
    "optimizer.zero_grad()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 04 - Backpropagation - Theory With Example\n",
    "\n",
    "- Examples, concepts and how to calculate gradients with it;\n",
    "- Chain rule: dz/dx = dz/dy . dy/dx\n",
    "- Computational graph - compute local gradients;\n",
    "\n",
    "1) Forward pass: compute loss;\n",
    "\n",
    "2) Compute local gradients;\n",
    "\n",
    "3) Backward pass: compute dLoss/dWeights using the chain rule;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., grad_fn=<PowBackward0>)\n",
      "tensor(-2.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "\n",
    "w = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "#forward pass and compute the loss\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "print(loss)\n",
    "\n",
    "# backward pass\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "# update weights, next forward and backwards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 05 - Gradient Descent with Autograd and Backpropagation\n",
    "\n",
    "- Concrete example on how to optimize our model with automatic gradient computation using PyTorch autograd package;\n",
    "\n",
    "- Implementing linear regression:\n",
    "    - 1) Prediction: manually;\n",
    "    - 1) Gradients computation: manually;\n",
    "    - 1) Loss computation: manually;\n",
    "    - 1) Parameter updates: manually;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "1: w = 1.200, loss = 30.00000000\n",
      "2: w = 1.680, loss = 4.79999924\n",
      "3: w = 1.872, loss = 0.76800019\n",
      "4: w = 1.949, loss = 0.12288000\n",
      "5: w = 1.980, loss = 0.01966083\n",
      "6: w = 1.992, loss = 0.00314574\n",
      "7: w = 1.997, loss = 0.00050331\n",
      "8: w = 1.999, loss = 0.00008053\n",
      "9: w = 1.999, loss = 0.00001288\n",
      "10: w = 2.000, loss = 0.00000206\n",
      "11: w = 2.000, loss = 0.00000033\n",
      "12: w = 2.000, loss = 0.00000005\n",
      "13: w = 2.000, loss = 0.00000001\n",
      "14: w = 2.000, loss = 0.00000000\n",
      "Prediction before training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# logistic regression implementation\n",
    "import numpy as np\n",
    "\n",
    "# f = x * w\n",
    "x = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "w = 0.0 # weights\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE (mean square error)\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE - 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x -y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 14\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    # gradients\n",
    "    dw = gradient(x, y, y_pred)\n",
    "    \n",
    "    # update weights (go in the negative direction of the training data)\n",
    "    w -= learning_rate * dw\n",
    "    \n",
    "    # print information\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'{epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - \n",
    "    - 2) Prediction: manually;\n",
    "    - 2) Gradients computation: autograd;\n",
    "    - 2) Loss computation: manually;\n",
    "    - 2) Parameter updates: manually;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "1: w = 0.300, loss = 30.00000000\n",
      "11: w = 1.665, loss = 1.16278565\n",
      "21: w = 1.934, loss = 0.04506890\n",
      "31: w = 1.987, loss = 0.00174685\n",
      "41: w = 1.997, loss = 0.00006770\n",
      "51: w = 1.999, loss = 0.00000262\n",
      "61: w = 2.000, loss = 0.00000010\n",
      "71: w = 2.000, loss = 0.00000000\n",
      "Prediction before training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# logistic regression implementation\n",
    "import torch\n",
    "\n",
    "# f = x * w\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    " # weight\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# loss = MSE (mean square error)\n",
    "def loss(y, y_predicted):\n",
    "    return ((y_predicted - y)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE - 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x -y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 80 # needs more iterations\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dx\n",
    "    \n",
    "    # update weights (go in the negative direction of the training data)\n",
    "    with torch.no_grad():\n",
    "        w -= learning_rate * w.grad\n",
    "        \n",
    "    # zero gradients\n",
    "    w.grad.zero_()\n",
    "    \n",
    "    # print information\n",
    "    if epoch % 10 == 0: # every 10th step\n",
    "        print(f'{epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 06 -  Training Pipeline: Model, Loss, and Optimizer\n",
    "\n",
    "- \n",
    "    - 3) Prediction: manually;\n",
    "    - 3) Gradients computation: autograd;\n",
    "    - 3) Loss computation: PyTorch loss;\n",
    "    - 3) Parameter updates: PyTorch optimizer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction before training: f(5) = 0.000\n",
      "1: w = 0.300, loss = 30.00000000\n",
      "11: w = 1.665, loss = 1.16278565\n",
      "21: w = 1.934, loss = 0.04506890\n",
      "31: w = 1.987, loss = 0.00174685\n",
      "41: w = 1.997, loss = 0.00006770\n",
      "51: w = 1.999, loss = 0.00000262\n",
      "61: w = 2.000, loss = 0.00000010\n",
      "71: w = 2.000, loss = 0.00000000\n",
      "Prediction before training: f(5) = 10.000\n"
     ]
    }
   ],
   "source": [
    "# 1) Design model (input, output size, forward pass)\n",
    "# 2) Contruct loss and optimizer\n",
    "# 3) Training loop\n",
    "#    - forward pass: compute prediction\n",
    "#    - backward pass: gradients\n",
    "#    - update weights\n",
    "\n",
    "import torch.nn as nn # use some functions from neural networks\n",
    "\n",
    "# f = x * w\n",
    "x = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "\n",
    " # weight\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "# gradient\n",
    "# MSE - 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x -y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 80 # needs more iterations\n",
    "\n",
    "# loss\n",
    "# exactly what we implemented before, but now with torch.nn function\n",
    "loss = nn.MSELoss() # mean root square\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate) # stocastic gradient descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(x)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dx\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "        \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # print information\n",
    "    if epoch % 10 == 0: # every 10th step\n",
    "        print(f'{epoch + 1}: w = {w:.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction before training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "    - 4) Prediction: pytorch model;\n",
    "    - 4) Gradients computation: autograd;\n",
    "    - 4) Loss computation: PyTorch loss;\n",
    "    - 4) Parameter updates: PyTorch optimizer;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = 4.361\n",
      "1: w = 1.152, loss = 10.74855423\n",
      "41: w = 2.041, loss = 0.00248790\n",
      "81: w = 2.037, loss = 0.00195354\n",
      "121: w = 2.033, loss = 0.00153690\n",
      "161: w = 2.029, loss = 0.00120913\n",
      "201: w = 2.026, loss = 0.00095126\n",
      "241: w = 2.023, loss = 0.00074839\n",
      "281: w = 2.020, loss = 0.00058877\n",
      "321: w = 2.018, loss = 0.00046321\n",
      "361: w = 2.016, loss = 0.00036441\n",
      "401: w = 2.014, loss = 0.00028669\n",
      "441: w = 2.012, loss = 0.00022556\n",
      "481: w = 2.011, loss = 0.00017745\n",
      "Prediction before training: f(5) = 10.022\n"
     ]
    }
   ],
   "source": [
    "# f = x * w\n",
    "x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features) # 4 1\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# gradient\n",
    "# MSE - 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x -y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 500 # needs more iterations\n",
    "\n",
    "# loss\n",
    "# exactly what we implemented before, but now with torch.nn function\n",
    "loss = nn.MSELoss() # mean root square\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # stocastic gradient descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dx\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "        \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # print information\n",
    "    if epoch % 40 == 0: # every 10th step\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'{epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 1\n",
      "Prediction before training: f(5) = -1.914\n",
      "1: w = -0.142, loss = 40.14307404\n",
      "41: w = 1.628, loss = 0.19977194\n",
      "81: w = 1.671, loss = 0.15715234\n",
      "121: w = 1.708, loss = 0.12363625\n",
      "161: w = 1.741, loss = 0.09726823\n",
      "201: w = 1.770, loss = 0.07652378\n",
      "241: w = 1.796, loss = 0.06020353\n",
      "281: w = 1.819, loss = 0.04736383\n",
      "321: w = 1.840, loss = 0.03726248\n",
      "361: w = 1.858, loss = 0.02931543\n",
      "401: w = 1.874, loss = 0.02306333\n",
      "441: w = 1.888, loss = 0.01814459\n",
      "481: w = 1.901, loss = 0.01427486\n",
      "Prediction before training: f(5) = 9.807\n"
     ]
    }
   ],
   "source": [
    "# f = x * w\n",
    "x = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "\n",
    "x_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "print(n_samples, n_features) # 4 1\n",
    "\n",
    "input_size = n_features\n",
    "output_size = n_features\n",
    "\n",
    "# creating a pytorch model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        # define layers\n",
    "        self.lin = nn.Linear(input_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.lin(x)\n",
    "    \n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# gradient\n",
    "# MSE - 1/N * (w*x - y)**2\n",
    "# dJ/dw = 1/N 2x (w*x -y)\n",
    "def gradient(x, y, y_predicted):\n",
    "    return np.dot(2*x, y_predicted - y).mean()\n",
    "\n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')\n",
    "\n",
    "# training\n",
    "learning_rate = 0.01\n",
    "n_iters = 500 # needs more iterations\n",
    "\n",
    "# loss\n",
    "# exactly what we implemented before, but now with torch.nn function\n",
    "loss = nn.MSELoss() # mean root square\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # stocastic gradient descent\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(y, y_pred)\n",
    "    \n",
    "    # gradients = backward pass\n",
    "    l.backward() # dl/dx\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "        \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # print information\n",
    "    if epoch % 40 == 0: # every 10th step\n",
    "        [w, b] = model.parameters()\n",
    "        print(f'{epoch + 1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
    "        \n",
    "print(f'Prediction before training: f(5) = {model(x_test).item():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 07 - Linear Regression\n",
    "\n",
    "- 1) Design model (input, output size, forward pass)\n",
    "- 2) Contruct loss and optimizer\n",
    "- 3) Training loop\n",
    "    - forward pass: compute prediction\n",
    "    - backward pass: gradients\n",
    "    - update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, loss = 563.8965\n",
      "epoch: 200, loss = 342.5907\n",
      "epoch: 300, loss = 333.0102\n",
      "epoch: 400, loss = 332.5873\n",
      "epoch: 500, loss = 332.5685\n",
      "epoch: 600, loss = 332.5676\n",
      "epoch: 700, loss = 332.5675\n",
      "epoch: 800, loss = 332.5675\n",
      "epoch: 900, loss = 332.5676\n",
      "epoch: 1000, loss = 332.5676\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 0) prepare data\n",
    "x_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise=20, random_state=1)\n",
    "\n",
    "x = torch.from_numpy(x_numpy.astype(np.float32))\n",
    "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
    "# make one column vector\n",
    "y = y.view(y.shape[0], 1)\n",
    "\n",
    "n_samples, n_features = x.shape\n",
    "\n",
    "# 1) model\n",
    "input_size = n_features\n",
    "output_size = 1\n",
    "model = nn.Linear(input_size, output_size)\n",
    "\n",
    "# 2) loss and optimizer\n",
    "learning_rate = 0.01\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "\n",
    "# 3) training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(x)\n",
    "    loss = criterion(y_predicted, y)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # update\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss = {loss.item():.4f}')\n",
    "        \n",
    "# plot \n",
    "predicted = model(x).detach().numpy()\n",
    "plt.plot(x_numpy, y_numpy,'ro')\n",
    "plt.plot(x_numpy, predicted, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 08 - Logistic Regression\n",
    "\n",
    "- 1) Design model (input, output size, forward pass)\n",
    "- 2) Construct loss and optimizer\n",
    "- 3) Training loop\n",
    "    - forward pass: compute prediction\n",
    "    - backward pass: gradients\n",
    "    - update weights\n",
    "    \n",
    "- Similar to Linear regression, just have to ajust the model and the loss function;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569 30\n",
      "epoch: 100, loss = 0.2374\n",
      "epoch: 200, loss = 0.1739\n",
      "epoch: 300, loss = 0.1456\n",
      "epoch: 400, loss = 0.1287\n",
      "epoch: 500, loss = 0.1173\n",
      "epoch: 600, loss = 0.1089\n",
      "epoch: 700, loss = 0.1025\n",
      "epoch: 800, loss = 0.0973\n",
      "epoch: 900, loss = 0.0930\n",
      "epoch: 1000, loss = 0.0894\n",
      "accuracy = 0.9211\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# binary classification dataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # binary classification dataset\n",
    "from sklearn.model_selection import train_test_split # split test / train\n",
    "\n",
    "# 0) prepare data ---------------------------------------------------------------\n",
    "# binary classification problem\n",
    "# predict cancer based on input features\n",
    "bc = datasets.load_breast_cancer()\n",
    "X, y = bc.data, bc.target\n",
    "\n",
    "n_samples, n_features = X.shape\n",
    "print(n_samples, n_features)\n",
    "# 569 samples and 30 different features\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234) # test_size 20%\n",
    "\n",
    "#scale \n",
    "sc      = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test  = sc.transform(X_test)\n",
    "\n",
    "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
    "X_test  = torch.from_numpy(X_test.astype(np.float32))\n",
    "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
    "y_test  = torch.from_numpy(y_test.astype(np.float32))\n",
    "\n",
    "# reshape tensor\n",
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test  = y_test.view(y_test.shape[0], 1)\n",
    "\n",
    "\n",
    "# 1) model ----------------------------------------------------------------------\n",
    "# f = wx + b, sigmoid at the end\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, n_input_features):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(n_input_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y_predicted = torch.sigmoid(self.linear(x))\n",
    "        return y_predicted\n",
    "    \n",
    "model = LogisticRegression(n_features)\n",
    "\n",
    "# 2) loss and optimizer ---------------------------------------------------------\n",
    "learning_rate = 0.01\n",
    "# BCE - binary cross entropy loss\n",
    "criterion = nn.BCELoss()\n",
    "# SGD - stocastic gradient descent\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 3) training loop --------------------------------------------------------------\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # forward pass and loss\n",
    "    y_predicted = model(X_train)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    \n",
    "    # updates\n",
    "    optimizer.step()\n",
    "    \n",
    "    # zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'epoch: {epoch + 1}, loss = {loss.item():.4f}')\n",
    "        \n",
    "with torch.no_grad():\n",
    "    y_predicted = model(X_test)\n",
    "    y_predicted_cls = y_predicted.round()\n",
    "    acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
    "    print(f'accuracy = {acc:.4f}')\n",
    "\n",
    "plt.plot(X, y, 'bo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 09 - Dataset and DataLoader - Batch Training\n",
    "\n",
    "- Pytorch dataset and data loader classes;\n",
    "- Before, we loaded the dataset as below, from a csv file;\n",
    "- And the trainig loop that looped over the number of epochs;\n",
    "- And optimized our model based on the whole data set;\n",
    "- This might be very time consuming if we do the calculation on this large data!\n",
    "\n",
    "- We can devide the samples into smaller **batches**;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# before - time consuming\\ndata = np.loadtxt('wine.csv')\\n\\n# training loop\\nfor epoch in range(1000):\\n    x, y = data\\n    # forward + backward + weight updates\\n\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# before - time consuming\n",
    "data = np.loadtxt('wine.csv')\n",
    "\n",
    "# training loop\n",
    "for epoch in range(1000):\n",
    "    x, y = data\n",
    "    # forward + backward + weight updates\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# after- optimized \\n\\n# training loop\\nfor epoch in range(1000):\\n    # loop over all batches\\n    for i in range(total_batches):\\n        x_batch, y_batch = ...\\n'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# after- optimized \n",
    "\n",
    "# training loop\n",
    "for epoch in range(1000):\n",
    "    # loop over all batches\n",
    "    for i in range(total_batches):\n",
    "        x_batch, y_batch = ...\n",
    "        \n",
    "# --> use DataSet and DataLoader to load wine.csv\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **epoch** = 1 forward and backward pass of ALL training samples;\n",
    "- **batch_size** = number of training samples in one forward and backward pass;\n",
    "- **number of iterations** = number of passes, each pass using [batch_size] number of samples;\n",
    "- e.g. 100 samples, batch_size=20 --> 100/20 = 5 iterations for 1 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03]) tensor([1.])\n",
      "tensor([1.4230e+01, 1.7100e+00, 2.4300e+00, 1.5600e+01, 1.2700e+02, 2.8000e+00,\n",
      "        3.0600e+00, 2.8000e-01, 2.2900e+00, 5.6400e+00, 1.0400e+00, 3.9200e+00,\n",
      "        1.0650e+03]) tensor([1.])\n",
      "178 45\n",
      "epoch = 1/2, step = 5/45, inputs = torch.Size([4, 13])\n",
      "epoch = 1/2, step = 10/45, inputs = torch.Size([4, 13])\n",
      "epoch = 1/2, step = 15/45, inputs = torch.Size([4, 13])\n",
      "epoch = 1/2, step = 20/45, inputs = torch.Size([4, 13])\n",
      "epoch = 1/2, step = 25/45, inputs = torch.Size([4, 13])\n",
      "epoch = 1/2, step = 30/45, inputs = torch.Size([4, 13])\n",
      "epoch = 1/2, step = 35/45, inputs = torch.Size([4, 13])\n",
      "epoch = 1/2, step = 40/45, inputs = torch.Size([4, 13])\n",
      "epoch = 1/2, step = 45/45, inputs = torch.Size([2, 13])\n",
      "epoch = 2/2, step = 5/45, inputs = torch.Size([4, 13])\n",
      "epoch = 2/2, step = 10/45, inputs = torch.Size([4, 13])\n",
      "epoch = 2/2, step = 15/45, inputs = torch.Size([4, 13])\n",
      "epoch = 2/2, step = 20/45, inputs = torch.Size([4, 13])\n",
      "epoch = 2/2, step = 25/45, inputs = torch.Size([4, 13])\n",
      "epoch = 2/2, step = 30/45, inputs = torch.Size([4, 13])\n",
      "epoch = 2/2, step = 35/45, inputs = torch.Size([4, 13])\n",
      "epoch = 2/2, step = 40/45, inputs = torch.Size([4, 13])\n",
      "epoch = 2/2, step = 45/45, inputs = torch.Size([2, 13])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import math\n",
    "\n",
    "class WineDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # data loading\n",
    "        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)\n",
    "        self.x = torch.from_numpy(xy[:, 1:])\n",
    "        self.y = torch.from_numpy(xy[:, [0]]) # n_samples, 1\n",
    "        self.n_samples = xy.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # dataset[0]\n",
    "        return self.x[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        # len(dataset)\n",
    "        return self.n_samples\n",
    "    \n",
    "dataset = WineDataset()\n",
    "first_data = dataset[0]\n",
    "features, labels = first_data\n",
    "print(features, labels)\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "datatiter = iter(dataloader)\n",
    "data = datatiter.next()\n",
    "print(features, labels)\n",
    "\n",
    "# training loop\n",
    "num_epochs = 2\n",
    "total_samples = len(dataset)\n",
    "n_iterations = math.ceil(total_samples / 4)\n",
    "print(total_samples, n_iterations) # 178 45\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, labels) in enumerate(dataloader):\n",
    "        # forward backward, update\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f'epoch = {epoch + 1}/{num_epochs}, step = {i + 1}/{n_iterations}, inputs = {inputs.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch Tutorial 10 - Dataset Transforms\n",
    "\n",
    "- Transform: convert images or numpy arrays to tensors;\n",
    "- Transforms can be applied to PIL images, tensors, ndarrays, or custom data during creation of the DataSet;\n",
    "- **On images**: CenterCrop, Grayscale, Pad, RandomAffine, RandomCrop, RandomHorizontalFlip, RandomRotation, Resize, Scale;\n",
    "- **On tensors**: LinearTransformation, Normalize, RandomErasing;\n",
    "- **Conversion**:\n",
    "    - ToPILImage: from tensor or ndarray;\n",
    "    - ToTensor: from numpy.ndarray or PILImage;\n",
    "- **Generic**: use lambda;\n",
    "- **Custom**: write own class, compose multiple transforms;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Compose multiple transforms\n",
    "---------------------------\n",
    "composed = transforms.Compose([Rescale(256),\n",
    "                               RandomCrop(224)])\n",
    "                               \n",
    "torchvision.transforms.ReScale(256)\n",
    "torchvision.transforms.ToTensor()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python38564bitbasecondab918388200e74c19bbf7a29e3542111d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
